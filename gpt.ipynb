{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "torch.device('mps') # use GPU if available\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('vocab size:', vocab_size)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 59, 54, 39]\n",
      "dupa\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: ''.join([itos[ch] for ch in x])\n",
    "\n",
    "print(encode(\"dupa\"))\n",
    "print(decode(encode(\"dupa\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "valid_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the context is F, the target is i\n",
      "When the context is Fi, the target is r\n",
      "When the context is Fir, the target is s\n",
      "When the context is Firs, the target is t\n",
      "When the context is First, the target is  \n",
      "When the context is First , the target is C\n",
      "When the context is First C, the target is i\n",
      "When the context is First Ci, the target is t\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'When the context is {decode(context.tolist())}, the target is {decode([target.tolist()])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n",
      "When the context is L, the target is e\n",
      "When the context is Le, the target is t\n",
      "When the context is Let, the target is '\n",
      "When the context is Let', the target is s\n",
      "When the context is Let's, the target is  \n",
      "When the context is Let's , the target is h\n",
      "When the context is Let's h, the target is e\n",
      "When the context is Let's he, the target is a\n",
      "When the context is f, the target is o\n",
      "When the context is fo, the target is r\n",
      "When the context is for, the target is  \n",
      "When the context is for , the target is t\n",
      "When the context is for t, the target is h\n",
      "When the context is for th, the target is a\n",
      "When the context is for tha, the target is t\n",
      "When the context is for that, the target is  \n",
      "When the context is n, the target is t\n",
      "When the context is nt, the target is  \n",
      "When the context is nt , the target is t\n",
      "When the context is nt t, the target is h\n",
      "When the context is nt th, the target is a\n",
      "When the context is nt tha, the target is t\n",
      "When the context is nt that, the target is  \n",
      "When the context is nt that , the target is h\n",
      "When the context is M, the target is E\n",
      "When the context is ME, the target is O\n",
      "When the context is MEO, the target is :\n",
      "When the context is MEO:, the target is \n",
      "\n",
      "When the context is MEO:\n",
      ", the target is I\n",
      "When the context is MEO:\n",
      "I, the target is  \n",
      "When the context is MEO:\n",
      "I , the target is p\n",
      "When the context is MEO:\n",
      "I p, the target is a\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else valid_data\n",
    "    ix = torch.randint(len(data) - block_size , (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b,:t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f'When the context is {decode(context.tolist())}, the target is {decode([target.tolist()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (batch_size, block_size, vocab_size) (B, T, C) (batch time channel)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits , loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=1) # B, C\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, 1) # B, 1\n",
    "            # add the new token to the sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # B, T+1\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0 is 3.6924993991851807\n",
      "Loss at step 10 is 3.703143358230591\n",
      "Loss at step 20 is 3.5420799255371094\n",
      "Loss at step 30 is 3.6378514766693115\n",
      "Loss at step 40 is 3.654754638671875\n",
      "Loss at step 50 is 3.638540267944336\n",
      "Loss at step 60 is 3.634798288345337\n",
      "Loss at step 70 is 3.546691417694092\n",
      "Loss at step 80 is 3.6222987174987793\n",
      "Loss at step 90 is 3.597193956375122\n",
      "Loss at step 100 is 3.6769022941589355\n",
      "Loss at step 110 is 3.6126725673675537\n",
      "Loss at step 120 is 3.5891876220703125\n",
      "Loss at step 130 is 3.53181791305542\n",
      "Loss at step 140 is 3.469095468521118\n",
      "Loss at step 150 is 3.425056219100952\n",
      "Loss at step 160 is 3.67372989654541\n",
      "Loss at step 170 is 3.5732293128967285\n",
      "Loss at step 180 is 3.454685926437378\n",
      "Loss at step 190 is 3.475454092025757\n",
      "Loss at step 200 is 3.4926655292510986\n",
      "Loss at step 210 is 3.506186008453369\n",
      "Loss at step 220 is 3.451472759246826\n",
      "Loss at step 230 is 3.4598655700683594\n",
      "Loss at step 240 is 3.476881742477417\n",
      "Loss at step 250 is 3.480024576187134\n",
      "Loss at step 260 is 3.464306116104126\n",
      "Loss at step 270 is 3.3898727893829346\n",
      "Loss at step 280 is 3.4497509002685547\n",
      "Loss at step 290 is 3.4901843070983887\n",
      "Loss at step 300 is 3.4855854511260986\n",
      "Loss at step 310 is 3.493288278579712\n",
      "Loss at step 320 is 3.363356351852417\n",
      "Loss at step 330 is 3.397303819656372\n",
      "Loss at step 340 is 3.3594202995300293\n",
      "Loss at step 350 is 3.495753765106201\n",
      "Loss at step 360 is 3.346740961074829\n",
      "Loss at step 370 is 3.3412704467773438\n",
      "Loss at step 380 is 3.3451335430145264\n",
      "Loss at step 390 is 3.354836940765381\n",
      "Loss at step 400 is 3.373920440673828\n",
      "Loss at step 410 is 3.3736581802368164\n",
      "Loss at step 420 is 3.3055131435394287\n",
      "Loss at step 430 is 3.456294059753418\n",
      "Loss at step 440 is 3.3395164012908936\n",
      "Loss at step 450 is 3.321563243865967\n",
      "Loss at step 460 is 3.357445478439331\n",
      "Loss at step 470 is 3.2839066982269287\n",
      "Loss at step 480 is 3.3372979164123535\n",
      "Loss at step 490 is 3.1944258213043213\n",
      "Loss at step 500 is 3.3374476432800293\n",
      "Loss at step 510 is 3.3406598567962646\n",
      "Loss at step 520 is 3.222597122192383\n",
      "Loss at step 530 is 3.3314146995544434\n",
      "Loss at step 540 is 3.331284523010254\n",
      "Loss at step 550 is 3.362391471862793\n",
      "Loss at step 560 is 3.1740188598632812\n",
      "Loss at step 570 is 3.177072763442993\n",
      "Loss at step 580 is 3.2766284942626953\n",
      "Loss at step 590 is 3.2156014442443848\n",
      "Loss at step 600 is 3.225123882293701\n",
      "Loss at step 610 is 3.2857754230499268\n",
      "Loss at step 620 is 3.2241358757019043\n",
      "Loss at step 630 is 3.3311963081359863\n",
      "Loss at step 640 is 3.287376642227173\n",
      "Loss at step 650 is 3.2359459400177\n",
      "Loss at step 660 is 3.219647169113159\n",
      "Loss at step 670 is 3.1789603233337402\n",
      "Loss at step 680 is 3.1751577854156494\n",
      "Loss at step 690 is 3.2514007091522217\n",
      "Loss at step 700 is 3.2503604888916016\n",
      "Loss at step 710 is 3.2081098556518555\n",
      "Loss at step 720 is 3.1328508853912354\n",
      "Loss at step 730 is 3.1158998012542725\n",
      "Loss at step 740 is 3.096991539001465\n",
      "Loss at step 750 is 3.2212252616882324\n",
      "Loss at step 760 is 3.1999313831329346\n",
      "Loss at step 770 is 3.1256282329559326\n",
      "Loss at step 780 is 3.194880723953247\n",
      "Loss at step 790 is 3.157536268234253\n",
      "Loss at step 800 is 3.1358368396759033\n",
      "Loss at step 810 is 3.1034724712371826\n",
      "Loss at step 820 is 3.2182161808013916\n",
      "Loss at step 830 is 3.0568995475769043\n",
      "Loss at step 840 is 3.1390771865844727\n",
      "Loss at step 850 is 3.1797432899475098\n",
      "Loss at step 860 is 3.09885835647583\n",
      "Loss at step 870 is 3.1141233444213867\n",
      "Loss at step 880 is 3.099656820297241\n",
      "Loss at step 890 is 3.16512393951416\n",
      "Loss at step 900 is 3.192910671234131\n",
      "Loss at step 910 is 3.165006399154663\n",
      "Loss at step 920 is 3.116950511932373\n",
      "Loss at step 930 is 3.1530752182006836\n",
      "Loss at step 940 is 3.1451358795166016\n",
      "Loss at step 950 is 3.1085293292999268\n",
      "Loss at step 960 is 3.0830273628234863\n",
      "Loss at step 970 is 3.02162766456604\n",
      "Loss at step 980 is 3.109046459197998\n",
      "Loss at step 990 is 2.9678921699523926\n",
      "Loss at step 1000 is 3.076063632965088\n",
      "Loss at step 1010 is 3.042084217071533\n",
      "Loss at step 1020 is 3.106255292892456\n",
      "Loss at step 1030 is 3.167919635772705\n",
      "Loss at step 1040 is 3.0844054222106934\n",
      "Loss at step 1050 is 3.077044725418091\n",
      "Loss at step 1060 is 3.160936117172241\n",
      "Loss at step 1070 is 3.0394039154052734\n",
      "Loss at step 1080 is 3.0070419311523438\n",
      "Loss at step 1090 is 3.0886001586914062\n",
      "Loss at step 1100 is 3.0919909477233887\n",
      "Loss at step 1110 is 3.083101749420166\n",
      "Loss at step 1120 is 3.0732827186584473\n",
      "Loss at step 1130 is 2.86140775680542\n",
      "Loss at step 1140 is 3.026740312576294\n",
      "Loss at step 1150 is 3.021716833114624\n",
      "Loss at step 1160 is 3.035919427871704\n",
      "Loss at step 1170 is 3.0672590732574463\n",
      "Loss at step 1180 is 2.931455373764038\n",
      "Loss at step 1190 is 2.971773624420166\n",
      "Loss at step 1200 is 2.996506452560425\n",
      "Loss at step 1210 is 2.9841580390930176\n",
      "Loss at step 1220 is 2.9585227966308594\n",
      "Loss at step 1230 is 2.9626457691192627\n",
      "Loss at step 1240 is 3.1452138423919678\n",
      "Loss at step 1250 is 3.001258611679077\n",
      "Loss at step 1260 is 3.0190773010253906\n",
      "Loss at step 1270 is 3.0185184478759766\n",
      "Loss at step 1280 is 2.9400556087493896\n",
      "Loss at step 1290 is 2.9905624389648438\n",
      "Loss at step 1300 is 3.0103485584259033\n",
      "Loss at step 1310 is 2.971494197845459\n",
      "Loss at step 1320 is 2.9597790241241455\n",
      "Loss at step 1330 is 2.914747476577759\n",
      "Loss at step 1340 is 2.9659340381622314\n",
      "Loss at step 1350 is 3.0295653343200684\n",
      "Loss at step 1360 is 2.9333088397979736\n",
      "Loss at step 1370 is 3.0013527870178223\n",
      "Loss at step 1380 is 2.909108877182007\n",
      "Loss at step 1390 is 3.0586910247802734\n",
      "Loss at step 1400 is 2.966742515563965\n",
      "Loss at step 1410 is 2.854166269302368\n",
      "Loss at step 1420 is 2.940214157104492\n",
      "Loss at step 1430 is 3.0144479274749756\n",
      "Loss at step 1440 is 2.8099234104156494\n",
      "Loss at step 1450 is 2.982104778289795\n",
      "Loss at step 1460 is 2.9497458934783936\n",
      "Loss at step 1470 is 2.8607239723205566\n",
      "Loss at step 1480 is 2.9298946857452393\n",
      "Loss at step 1490 is 2.858261823654175\n",
      "Loss at step 1500 is 2.974670648574829\n",
      "Loss at step 1510 is 2.9013266563415527\n",
      "Loss at step 1520 is 2.865198850631714\n",
      "Loss at step 1530 is 2.904902219772339\n",
      "Loss at step 1540 is 2.900771379470825\n",
      "Loss at step 1550 is 2.8655948638916016\n",
      "Loss at step 1560 is 2.8769640922546387\n",
      "Loss at step 1570 is 3.018261671066284\n",
      "Loss at step 1580 is 2.9569766521453857\n",
      "Loss at step 1590 is 2.83506441116333\n",
      "Loss at step 1600 is 2.8123276233673096\n",
      "Loss at step 1610 is 2.9875924587249756\n",
      "Loss at step 1620 is 2.9039692878723145\n",
      "Loss at step 1630 is 2.8311591148376465\n",
      "Loss at step 1640 is 2.8715054988861084\n",
      "Loss at step 1650 is 2.9324662685394287\n",
      "Loss at step 1660 is 2.8318750858306885\n",
      "Loss at step 1670 is 2.849994659423828\n",
      "Loss at step 1680 is 2.958083152770996\n",
      "Loss at step 1690 is 2.733119010925293\n",
      "Loss at step 1700 is 2.8890902996063232\n",
      "Loss at step 1710 is 2.808647632598877\n",
      "Loss at step 1720 is 2.837263584136963\n",
      "Loss at step 1730 is 2.807217836380005\n",
      "Loss at step 1740 is 2.8844974040985107\n",
      "Loss at step 1750 is 2.884918212890625\n",
      "Loss at step 1760 is 2.915989637374878\n",
      "Loss at step 1770 is 2.8056933879852295\n",
      "Loss at step 1780 is 2.857316255569458\n",
      "Loss at step 1790 is 2.7821130752563477\n",
      "Loss at step 1800 is 2.923489570617676\n",
      "Loss at step 1810 is 2.7923245429992676\n",
      "Loss at step 1820 is 2.831519365310669\n",
      "Loss at step 1830 is 2.7451982498168945\n",
      "Loss at step 1840 is 2.814465045928955\n",
      "Loss at step 1850 is 2.8807711601257324\n",
      "Loss at step 1860 is 2.725151777267456\n",
      "Loss at step 1870 is 2.839512586593628\n",
      "Loss at step 1880 is 2.804008722305298\n",
      "Loss at step 1890 is 2.7043616771698\n",
      "Loss at step 1900 is 2.8452279567718506\n",
      "Loss at step 1910 is 2.7602901458740234\n",
      "Loss at step 1920 is 2.807467222213745\n",
      "Loss at step 1930 is 2.9047229290008545\n",
      "Loss at step 1940 is 2.8361449241638184\n",
      "Loss at step 1950 is 2.751671314239502\n",
      "Loss at step 1960 is 2.789576292037964\n",
      "Loss at step 1970 is 2.771209955215454\n",
      "Loss at step 1980 is 2.715484857559204\n",
      "Loss at step 1990 is 2.8723649978637695\n",
      "Loss at step 2000 is 2.6441850662231445\n",
      "Loss at step 2010 is 2.7784059047698975\n",
      "Loss at step 2020 is 2.7269821166992188\n",
      "Loss at step 2030 is 2.8141307830810547\n",
      "Loss at step 2040 is 2.7526652812957764\n",
      "Loss at step 2050 is 2.8085358142852783\n",
      "Loss at step 2060 is 2.786708116531372\n",
      "Loss at step 2070 is 2.6565101146698\n",
      "Loss at step 2080 is 2.703634023666382\n",
      "Loss at step 2090 is 2.755486249923706\n",
      "Loss at step 2100 is 2.8429203033447266\n",
      "Loss at step 2110 is 2.715585708618164\n",
      "Loss at step 2120 is 2.778036594390869\n",
      "Loss at step 2130 is 2.7020139694213867\n",
      "Loss at step 2140 is 2.6520321369171143\n",
      "Loss at step 2150 is 2.821829319000244\n",
      "Loss at step 2160 is 2.753995180130005\n",
      "Loss at step 2170 is 2.7244787216186523\n",
      "Loss at step 2180 is 2.817753314971924\n",
      "Loss at step 2190 is 2.830193519592285\n",
      "Loss at step 2200 is 2.985287666320801\n",
      "Loss at step 2210 is 2.720654249191284\n",
      "Loss at step 2220 is 2.7919704914093018\n",
      "Loss at step 2230 is 2.6796910762786865\n",
      "Loss at step 2240 is 2.6275384426116943\n",
      "Loss at step 2250 is 2.7637805938720703\n",
      "Loss at step 2260 is 2.702491044998169\n",
      "Loss at step 2270 is 2.6263790130615234\n",
      "Loss at step 2280 is 2.724808692932129\n",
      "Loss at step 2290 is 2.777966260910034\n",
      "Loss at step 2300 is 2.7373836040496826\n",
      "Loss at step 2310 is 2.7169394493103027\n",
      "Loss at step 2320 is 2.7285971641540527\n",
      "Loss at step 2330 is 2.700979232788086\n",
      "Loss at step 2340 is 2.756679058074951\n",
      "Loss at step 2350 is 2.7459216117858887\n",
      "Loss at step 2360 is 2.6716785430908203\n",
      "Loss at step 2370 is 2.7358310222625732\n",
      "Loss at step 2380 is 2.7546987533569336\n",
      "Loss at step 2390 is 2.717942714691162\n",
      "Loss at step 2400 is 2.6757547855377197\n",
      "Loss at step 2410 is 2.7273716926574707\n",
      "Loss at step 2420 is 2.678840160369873\n",
      "Loss at step 2430 is 2.6719255447387695\n",
      "Loss at step 2440 is 2.6660847663879395\n",
      "Loss at step 2450 is 2.636714220046997\n",
      "Loss at step 2460 is 2.65645432472229\n",
      "Loss at step 2470 is 2.8173816204071045\n",
      "Loss at step 2480 is 2.8084716796875\n",
      "Loss at step 2490 is 2.643280029296875\n",
      "Loss at step 2500 is 2.765570878982544\n",
      "Loss at step 2510 is 2.604008674621582\n",
      "Loss at step 2520 is 2.7104456424713135\n",
      "Loss at step 2530 is 2.629582643508911\n",
      "Loss at step 2540 is 2.747389793395996\n",
      "Loss at step 2550 is 2.806579351425171\n",
      "Loss at step 2560 is 2.74330472946167\n",
      "Loss at step 2570 is 2.5994045734405518\n",
      "Loss at step 2580 is 2.783146858215332\n",
      "Loss at step 2590 is 2.6733603477478027\n",
      "Loss at step 2600 is 2.5476877689361572\n",
      "Loss at step 2610 is 2.6970605850219727\n",
      "Loss at step 2620 is 2.743199110031128\n",
      "Loss at step 2630 is 2.6329336166381836\n",
      "Loss at step 2640 is 2.731407880783081\n",
      "Loss at step 2650 is 2.529907464981079\n",
      "Loss at step 2660 is 2.6315906047821045\n",
      "Loss at step 2670 is 2.568603992462158\n",
      "Loss at step 2680 is 2.569378137588501\n",
      "Loss at step 2690 is 2.558305025100708\n",
      "Loss at step 2700 is 2.5318117141723633\n",
      "Loss at step 2710 is 2.6380727291107178\n",
      "Loss at step 2720 is 2.6287620067596436\n",
      "Loss at step 2730 is 2.6815123558044434\n",
      "Loss at step 2740 is 2.5831408500671387\n",
      "Loss at step 2750 is 2.725632667541504\n",
      "Loss at step 2760 is 2.6729214191436768\n",
      "Loss at step 2770 is 2.581826686859131\n",
      "Loss at step 2780 is 2.6192314624786377\n",
      "Loss at step 2790 is 2.7282164096832275\n",
      "Loss at step 2800 is 2.572080135345459\n",
      "Loss at step 2810 is 2.7104873657226562\n",
      "Loss at step 2820 is 2.5816335678100586\n",
      "Loss at step 2830 is 2.568237781524658\n",
      "Loss at step 2840 is 2.619871139526367\n",
      "Loss at step 2850 is 2.5768752098083496\n",
      "Loss at step 2860 is 2.566800355911255\n",
      "Loss at step 2870 is 2.68703031539917\n",
      "Loss at step 2880 is 2.5649337768554688\n",
      "Loss at step 2890 is 2.6429193019866943\n",
      "Loss at step 2900 is 2.640331268310547\n",
      "Loss at step 2910 is 2.655949831008911\n",
      "Loss at step 2920 is 2.554042339324951\n",
      "Loss at step 2930 is 2.7005298137664795\n",
      "Loss at step 2940 is 2.6983635425567627\n",
      "Loss at step 2950 is 2.6269686222076416\n",
      "Loss at step 2960 is 2.711242914199829\n",
      "Loss at step 2970 is 2.57151460647583\n",
      "Loss at step 2980 is 2.6933555603027344\n",
      "Loss at step 2990 is 2.690886974334717\n",
      "Loss at step 3000 is 2.4826505184173584\n",
      "Loss at step 3010 is 2.5851504802703857\n",
      "Loss at step 3020 is 2.624755859375\n",
      "Loss at step 3030 is 2.5044829845428467\n",
      "Loss at step 3040 is 2.601811647415161\n",
      "Loss at step 3050 is 2.6639270782470703\n",
      "Loss at step 3060 is 2.6242973804473877\n",
      "Loss at step 3070 is 2.650829315185547\n",
      "Loss at step 3080 is 2.5402145385742188\n",
      "Loss at step 3090 is 2.693821907043457\n",
      "Loss at step 3100 is 2.6490871906280518\n",
      "Loss at step 3110 is 2.633295774459839\n",
      "Loss at step 3120 is 2.7239725589752197\n",
      "Loss at step 3130 is 2.5715909004211426\n",
      "Loss at step 3140 is 2.726332426071167\n",
      "Loss at step 3150 is 2.647382974624634\n",
      "Loss at step 3160 is 2.5007615089416504\n",
      "Loss at step 3170 is 2.6220383644104004\n",
      "Loss at step 3180 is 2.610992431640625\n",
      "Loss at step 3190 is 2.5310542583465576\n",
      "Loss at step 3200 is 2.6980862617492676\n",
      "Loss at step 3210 is 2.7559361457824707\n",
      "Loss at step 3220 is 2.7421727180480957\n",
      "Loss at step 3230 is 2.5708158016204834\n",
      "Loss at step 3240 is 2.563739776611328\n",
      "Loss at step 3250 is 2.6187007427215576\n",
      "Loss at step 3260 is 2.5008554458618164\n",
      "Loss at step 3270 is 2.609109878540039\n",
      "Loss at step 3280 is 2.5801913738250732\n",
      "Loss at step 3290 is 2.6125080585479736\n",
      "Loss at step 3300 is 2.6355061531066895\n",
      "Loss at step 3310 is 2.615260601043701\n",
      "Loss at step 3320 is 2.5049538612365723\n",
      "Loss at step 3330 is 2.7428290843963623\n",
      "Loss at step 3340 is 2.603027820587158\n",
      "Loss at step 3350 is 2.6273858547210693\n",
      "Loss at step 3360 is 2.6453335285186768\n",
      "Loss at step 3370 is 2.5411884784698486\n",
      "Loss at step 3380 is 2.60058331489563\n",
      "Loss at step 3390 is 2.545128345489502\n",
      "Loss at step 3400 is 2.466493606567383\n",
      "Loss at step 3410 is 2.734363317489624\n",
      "Loss at step 3420 is 2.4279966354370117\n",
      "Loss at step 3430 is 2.5241198539733887\n",
      "Loss at step 3440 is 2.5813069343566895\n",
      "Loss at step 3450 is 2.580738067626953\n",
      "Loss at step 3460 is 2.6262059211730957\n",
      "Loss at step 3470 is 2.584911584854126\n",
      "Loss at step 3480 is 2.50197434425354\n",
      "Loss at step 3490 is 2.5371015071868896\n",
      "Loss at step 3500 is 2.589265823364258\n",
      "Loss at step 3510 is 2.5301876068115234\n",
      "Loss at step 3520 is 2.501783847808838\n",
      "Loss at step 3530 is 2.6908862590789795\n",
      "Loss at step 3540 is 2.4790284633636475\n",
      "Loss at step 3550 is 2.496917486190796\n",
      "Loss at step 3560 is 2.5995993614196777\n",
      "Loss at step 3570 is 2.6189002990722656\n",
      "Loss at step 3580 is 2.7095634937286377\n",
      "Loss at step 3590 is 2.548675775527954\n",
      "Loss at step 3600 is 2.4769835472106934\n",
      "Loss at step 3610 is 2.570997714996338\n",
      "Loss at step 3620 is 2.538686990737915\n",
      "Loss at step 3630 is 2.5845792293548584\n",
      "Loss at step 3640 is 2.6693150997161865\n",
      "Loss at step 3650 is 2.6021993160247803\n",
      "Loss at step 3660 is 2.5594322681427\n",
      "Loss at step 3670 is 2.720710277557373\n",
      "Loss at step 3680 is 2.5143401622772217\n",
      "Loss at step 3690 is 2.5231773853302\n",
      "Loss at step 3700 is 2.5282483100891113\n",
      "Loss at step 3710 is 2.583706855773926\n",
      "Loss at step 3720 is 2.5482561588287354\n",
      "Loss at step 3730 is 2.6207172870635986\n",
      "Loss at step 3740 is 2.7093918323516846\n",
      "Loss at step 3750 is 2.6199870109558105\n",
      "Loss at step 3760 is 2.5192785263061523\n",
      "Loss at step 3770 is 2.5399537086486816\n",
      "Loss at step 3780 is 2.4978878498077393\n",
      "Loss at step 3790 is 2.6417088508605957\n",
      "Loss at step 3800 is 2.4829256534576416\n",
      "Loss at step 3810 is 2.5138118267059326\n",
      "Loss at step 3820 is 2.4857921600341797\n",
      "Loss at step 3830 is 2.570110321044922\n",
      "Loss at step 3840 is 2.5614945888519287\n",
      "Loss at step 3850 is 2.5159404277801514\n",
      "Loss at step 3860 is 2.4906487464904785\n",
      "Loss at step 3870 is 2.5519118309020996\n",
      "Loss at step 3880 is 2.633317708969116\n",
      "Loss at step 3890 is 2.495577335357666\n",
      "Loss at step 3900 is 2.474458932876587\n",
      "Loss at step 3910 is 2.4099297523498535\n",
      "Loss at step 3920 is 2.598172426223755\n",
      "Loss at step 3930 is 2.56172776222229\n",
      "Loss at step 3940 is 2.6714327335357666\n",
      "Loss at step 3950 is 2.628945827484131\n",
      "Loss at step 3960 is 2.589725971221924\n",
      "Loss at step 3970 is 2.4839813709259033\n",
      "Loss at step 3980 is 2.5716474056243896\n",
      "Loss at step 3990 is 2.48441481590271\n",
      "Loss at step 4000 is 2.4732911586761475\n",
      "Loss at step 4010 is 2.480409622192383\n",
      "Loss at step 4020 is 2.451716899871826\n",
      "Loss at step 4030 is 2.50272536277771\n",
      "Loss at step 4040 is 2.49589467048645\n",
      "Loss at step 4050 is 2.567734718322754\n",
      "Loss at step 4060 is 2.44978404045105\n",
      "Loss at step 4070 is 2.6367907524108887\n",
      "Loss at step 4080 is 2.5316741466522217\n",
      "Loss at step 4090 is 2.5547661781311035\n",
      "Loss at step 4100 is 2.611161231994629\n",
      "Loss at step 4110 is 2.5384795665740967\n",
      "Loss at step 4120 is 2.566880941390991\n",
      "Loss at step 4130 is 2.52937388420105\n",
      "Loss at step 4140 is 2.586102247238159\n",
      "Loss at step 4150 is 2.5216472148895264\n",
      "Loss at step 4160 is 2.571126699447632\n",
      "Loss at step 4170 is 2.540497064590454\n",
      "Loss at step 4180 is 2.5751798152923584\n",
      "Loss at step 4190 is 2.58376145362854\n",
      "Loss at step 4200 is 2.522033452987671\n",
      "Loss at step 4210 is 2.5052385330200195\n",
      "Loss at step 4220 is 2.5455641746520996\n",
      "Loss at step 4230 is 2.4839916229248047\n",
      "Loss at step 4240 is 2.4758858680725098\n",
      "Loss at step 4250 is 2.5136196613311768\n",
      "Loss at step 4260 is 2.5814783573150635\n",
      "Loss at step 4270 is 2.559968948364258\n",
      "Loss at step 4280 is 2.648379325866699\n",
      "Loss at step 4290 is 2.523358106613159\n",
      "Loss at step 4300 is 2.6331887245178223\n",
      "Loss at step 4310 is 2.5122623443603516\n",
      "Loss at step 4320 is 2.4479777812957764\n",
      "Loss at step 4330 is 2.520860433578491\n",
      "Loss at step 4340 is 2.5426106452941895\n",
      "Loss at step 4350 is 2.5634288787841797\n",
      "Loss at step 4360 is 2.4833600521087646\n",
      "Loss at step 4370 is 2.5288121700286865\n",
      "Loss at step 4380 is 2.6399788856506348\n",
      "Loss at step 4390 is 2.471453905105591\n",
      "Loss at step 4400 is 2.4180667400360107\n",
      "Loss at step 4410 is 2.634178400039673\n",
      "Loss at step 4420 is 2.4776089191436768\n",
      "Loss at step 4430 is 2.445899724960327\n",
      "Loss at step 4440 is 2.4660730361938477\n",
      "Loss at step 4450 is 2.5764291286468506\n",
      "Loss at step 4460 is 2.503124237060547\n",
      "Loss at step 4470 is 2.3361704349517822\n",
      "Loss at step 4480 is 2.605982542037964\n",
      "Loss at step 4490 is 2.501091480255127\n",
      "Loss at step 4500 is 2.6324105262756348\n",
      "Loss at step 4510 is 2.411250114440918\n",
      "Loss at step 4520 is 2.4526116847991943\n",
      "Loss at step 4530 is 2.616312265396118\n",
      "Loss at step 4540 is 2.526118278503418\n",
      "Loss at step 4550 is 2.5547749996185303\n",
      "Loss at step 4560 is 2.4717512130737305\n",
      "Loss at step 4570 is 2.563737630844116\n",
      "Loss at step 4580 is 2.518697738647461\n",
      "Loss at step 4590 is 2.5096466541290283\n",
      "Loss at step 4600 is 2.559250831604004\n",
      "Loss at step 4610 is 2.4028122425079346\n",
      "Loss at step 4620 is 2.497865676879883\n",
      "Loss at step 4630 is 2.5801432132720947\n",
      "Loss at step 4640 is 2.6227097511291504\n",
      "Loss at step 4650 is 2.5087363719940186\n",
      "Loss at step 4660 is 2.5456621646881104\n",
      "Loss at step 4670 is 2.5874075889587402\n",
      "Loss at step 4680 is 2.4203426837921143\n",
      "Loss at step 4690 is 2.503835439682007\n",
      "Loss at step 4700 is 2.553264617919922\n",
      "Loss at step 4710 is 2.5547478199005127\n",
      "Loss at step 4720 is 2.5075554847717285\n",
      "Loss at step 4730 is 2.60382080078125\n",
      "Loss at step 4740 is 2.4837825298309326\n",
      "Loss at step 4750 is 2.5242977142333984\n",
      "Loss at step 4760 is 2.4451851844787598\n",
      "Loss at step 4770 is 2.6398468017578125\n",
      "Loss at step 4780 is 2.563694715499878\n",
      "Loss at step 4790 is 2.6210968494415283\n",
      "Loss at step 4800 is 2.573803663253784\n",
      "Loss at step 4810 is 2.5038223266601562\n",
      "Loss at step 4820 is 2.4525015354156494\n",
      "Loss at step 4830 is 2.5837738513946533\n",
      "Loss at step 4840 is 2.6361336708068848\n",
      "Loss at step 4850 is 2.637500524520874\n",
      "Loss at step 4860 is 2.563694953918457\n",
      "Loss at step 4870 is 2.4910738468170166\n",
      "Loss at step 4880 is 2.611764669418335\n",
      "Loss at step 4890 is 2.43282151222229\n",
      "Loss at step 4900 is 2.434170961380005\n",
      "Loss at step 4910 is 2.506314992904663\n",
      "Loss at step 4920 is 2.5006167888641357\n",
      "Loss at step 4930 is 2.530977725982666\n",
      "Loss at step 4940 is 2.5123705863952637\n",
      "Loss at step 4950 is 2.458174228668213\n",
      "Loss at step 4960 is 2.4834177494049072\n",
      "Loss at step 4970 is 2.4894914627075195\n",
      "Loss at step 4980 is 2.5873119831085205\n",
      "Loss at step 4990 is 2.649104595184326\n",
      "Loss at step 5000 is 2.547431468963623\n",
      "Loss at step 5010 is 2.4783382415771484\n",
      "Loss at step 5020 is 2.522197723388672\n",
      "Loss at step 5030 is 2.4369142055511475\n",
      "Loss at step 5040 is 2.5013129711151123\n",
      "Loss at step 5050 is 2.4965200424194336\n",
      "Loss at step 5060 is 2.435352087020874\n",
      "Loss at step 5070 is 2.4605672359466553\n",
      "Loss at step 5080 is 2.424891233444214\n",
      "Loss at step 5090 is 2.5913166999816895\n",
      "Loss at step 5100 is 2.490119218826294\n",
      "Loss at step 5110 is 2.5604307651519775\n",
      "Loss at step 5120 is 2.3398280143737793\n",
      "Loss at step 5130 is 2.502563714981079\n",
      "Loss at step 5140 is 2.583880662918091\n",
      "Loss at step 5150 is 2.4341201782226562\n",
      "Loss at step 5160 is 2.3383774757385254\n",
      "Loss at step 5170 is 2.5050723552703857\n",
      "Loss at step 5180 is 2.4340734481811523\n",
      "Loss at step 5190 is 2.3485958576202393\n",
      "Loss at step 5200 is 2.441471815109253\n",
      "Loss at step 5210 is 2.4982974529266357\n",
      "Loss at step 5220 is 2.5583910942077637\n",
      "Loss at step 5230 is 2.467679500579834\n",
      "Loss at step 5240 is 2.6922268867492676\n",
      "Loss at step 5250 is 2.4608843326568604\n",
      "Loss at step 5260 is 2.4811816215515137\n",
      "Loss at step 5270 is 2.4499902725219727\n",
      "Loss at step 5280 is 2.5652050971984863\n",
      "Loss at step 5290 is 2.562570333480835\n",
      "Loss at step 5300 is 2.4756662845611572\n",
      "Loss at step 5310 is 2.5146543979644775\n",
      "Loss at step 5320 is 2.5818557739257812\n",
      "Loss at step 5330 is 2.407750129699707\n",
      "Loss at step 5340 is 2.5780067443847656\n",
      "Loss at step 5350 is 2.465162754058838\n",
      "Loss at step 5360 is 2.5335185527801514\n",
      "Loss at step 5370 is 2.5409748554229736\n",
      "Loss at step 5380 is 2.5584797859191895\n",
      "Loss at step 5390 is 2.4417147636413574\n",
      "Loss at step 5400 is 2.5663247108459473\n",
      "Loss at step 5410 is 2.4192092418670654\n",
      "Loss at step 5420 is 2.561110019683838\n",
      "Loss at step 5430 is 2.4830288887023926\n",
      "Loss at step 5440 is 2.426968574523926\n",
      "Loss at step 5450 is 2.592587471008301\n",
      "Loss at step 5460 is 2.585238218307495\n",
      "Loss at step 5470 is 2.653334140777588\n",
      "Loss at step 5480 is 2.532825469970703\n",
      "Loss at step 5490 is 2.58256459236145\n",
      "Loss at step 5500 is 2.6430463790893555\n",
      "Loss at step 5510 is 2.4691121578216553\n",
      "Loss at step 5520 is 2.5873680114746094\n",
      "Loss at step 5530 is 2.5404133796691895\n",
      "Loss at step 5540 is 2.491565704345703\n",
      "Loss at step 5550 is 2.5362658500671387\n",
      "Loss at step 5560 is 2.5137343406677246\n",
      "Loss at step 5570 is 2.3832101821899414\n",
      "Loss at step 5580 is 2.59673810005188\n",
      "Loss at step 5590 is 2.5833663940429688\n",
      "Loss at step 5600 is 2.5848987102508545\n",
      "Loss at step 5610 is 2.5085296630859375\n",
      "Loss at step 5620 is 2.4847211837768555\n",
      "Loss at step 5630 is 2.4924044609069824\n",
      "Loss at step 5640 is 2.4735794067382812\n",
      "Loss at step 5650 is 2.5046865940093994\n",
      "Loss at step 5660 is 2.558789014816284\n",
      "Loss at step 5670 is 2.5247161388397217\n",
      "Loss at step 5680 is 2.4074008464813232\n",
      "Loss at step 5690 is 2.4972641468048096\n",
      "Loss at step 5700 is 2.4757018089294434\n",
      "Loss at step 5710 is 2.458092451095581\n",
      "Loss at step 5720 is 2.479499578475952\n",
      "Loss at step 5730 is 2.676187038421631\n",
      "Loss at step 5740 is 2.5467889308929443\n",
      "Loss at step 5750 is 2.4568140506744385\n",
      "Loss at step 5760 is 2.499199151992798\n",
      "Loss at step 5770 is 2.5185353755950928\n",
      "Loss at step 5780 is 2.5937535762786865\n",
      "Loss at step 5790 is 2.5226199626922607\n",
      "Loss at step 5800 is 2.5483555793762207\n",
      "Loss at step 5810 is 2.508798837661743\n",
      "Loss at step 5820 is 2.469719171524048\n",
      "Loss at step 5830 is 2.4745709896087646\n",
      "Loss at step 5840 is 2.52315354347229\n",
      "Loss at step 5850 is 2.444319009780884\n",
      "Loss at step 5860 is 2.4495019912719727\n",
      "Loss at step 5870 is 2.473285675048828\n",
      "Loss at step 5880 is 2.4492850303649902\n",
      "Loss at step 5890 is 2.565781593322754\n",
      "Loss at step 5900 is 2.5255651473999023\n",
      "Loss at step 5910 is 2.4037201404571533\n",
      "Loss at step 5920 is 2.535951614379883\n",
      "Loss at step 5930 is 2.512754201889038\n",
      "Loss at step 5940 is 2.535128355026245\n",
      "Loss at step 5950 is 2.5334248542785645\n",
      "Loss at step 5960 is 2.3893356323242188\n",
      "Loss at step 5970 is 2.448923110961914\n",
      "Loss at step 5980 is 2.4702138900756836\n",
      "Loss at step 5990 is 2.529998779296875\n",
      "Loss at step 6000 is 2.4185891151428223\n",
      "Loss at step 6010 is 2.5146613121032715\n",
      "Loss at step 6020 is 2.5174543857574463\n",
      "Loss at step 6030 is 2.338839292526245\n",
      "Loss at step 6040 is 2.4308767318725586\n",
      "Loss at step 6050 is 2.504819631576538\n",
      "Loss at step 6060 is 2.536283493041992\n",
      "Loss at step 6070 is 2.5046803951263428\n",
      "Loss at step 6080 is 2.524028778076172\n",
      "Loss at step 6090 is 2.522773504257202\n",
      "Loss at step 6100 is 2.4795758724212646\n",
      "Loss at step 6110 is 2.4767472743988037\n",
      "Loss at step 6120 is 2.4759624004364014\n",
      "Loss at step 6130 is 2.5656161308288574\n",
      "Loss at step 6140 is 2.621748447418213\n",
      "Loss at step 6150 is 2.4423959255218506\n",
      "Loss at step 6160 is 2.507359027862549\n",
      "Loss at step 6170 is 2.5469162464141846\n",
      "Loss at step 6180 is 2.4185123443603516\n",
      "Loss at step 6190 is 2.585475444793701\n",
      "Loss at step 6200 is 2.5683584213256836\n",
      "Loss at step 6210 is 2.4668283462524414\n",
      "Loss at step 6220 is 2.397289514541626\n",
      "Loss at step 6230 is 2.5430896282196045\n",
      "Loss at step 6240 is 2.409151792526245\n",
      "Loss at step 6250 is 2.382398843765259\n",
      "Loss at step 6260 is 2.49739146232605\n",
      "Loss at step 6270 is 2.4857895374298096\n",
      "Loss at step 6280 is 2.574723482131958\n",
      "Loss at step 6290 is 2.380458354949951\n",
      "Loss at step 6300 is 2.505964994430542\n",
      "Loss at step 6310 is 2.557610273361206\n",
      "Loss at step 6320 is 2.3820199966430664\n",
      "Loss at step 6330 is 2.5645785331726074\n",
      "Loss at step 6340 is 2.3721885681152344\n",
      "Loss at step 6350 is 2.5580360889434814\n",
      "Loss at step 6360 is 2.5043094158172607\n",
      "Loss at step 6370 is 2.5661160945892334\n",
      "Loss at step 6380 is 2.4174234867095947\n",
      "Loss at step 6390 is 2.4432601928710938\n",
      "Loss at step 6400 is 2.5466854572296143\n",
      "Loss at step 6410 is 2.6256699562072754\n",
      "Loss at step 6420 is 2.4987688064575195\n",
      "Loss at step 6430 is 2.3384182453155518\n",
      "Loss at step 6440 is 2.5053374767303467\n",
      "Loss at step 6450 is 2.5215096473693848\n",
      "Loss at step 6460 is 2.4929358959198\n",
      "Loss at step 6470 is 2.5154669284820557\n",
      "Loss at step 6480 is 2.4546399116516113\n",
      "Loss at step 6490 is 2.560605764389038\n",
      "Loss at step 6500 is 2.4413199424743652\n",
      "Loss at step 6510 is 2.6003992557525635\n",
      "Loss at step 6520 is 2.435056447982788\n",
      "Loss at step 6530 is 2.534156322479248\n",
      "Loss at step 6540 is 2.5797669887542725\n",
      "Loss at step 6550 is 2.4094693660736084\n",
      "Loss at step 6560 is 2.4977195262908936\n",
      "Loss at step 6570 is 2.486804962158203\n",
      "Loss at step 6580 is 2.514512300491333\n",
      "Loss at step 6590 is 2.4191677570343018\n",
      "Loss at step 6600 is 2.510568618774414\n",
      "Loss at step 6610 is 2.5157594680786133\n",
      "Loss at step 6620 is 2.529993772506714\n",
      "Loss at step 6630 is 2.492994785308838\n",
      "Loss at step 6640 is 2.5394680500030518\n",
      "Loss at step 6650 is 2.437066078186035\n",
      "Loss at step 6660 is 2.44209885597229\n",
      "Loss at step 6670 is 2.4787793159484863\n",
      "Loss at step 6680 is 2.5761873722076416\n",
      "Loss at step 6690 is 2.495284080505371\n",
      "Loss at step 6700 is 2.423603057861328\n",
      "Loss at step 6710 is 2.406717300415039\n",
      "Loss at step 6720 is 2.512144088745117\n",
      "Loss at step 6730 is 2.5103580951690674\n",
      "Loss at step 6740 is 2.34428334236145\n",
      "Loss at step 6750 is 2.4405741691589355\n",
      "Loss at step 6760 is 2.5800352096557617\n",
      "Loss at step 6770 is 2.5506234169006348\n",
      "Loss at step 6780 is 2.4182868003845215\n",
      "Loss at step 6790 is 2.497572898864746\n",
      "Loss at step 6800 is 2.496175765991211\n",
      "Loss at step 6810 is 2.5274817943573\n",
      "Loss at step 6820 is 2.3532378673553467\n",
      "Loss at step 6830 is 2.3991661071777344\n",
      "Loss at step 6840 is 2.51780366897583\n",
      "Loss at step 6850 is 2.4602935314178467\n",
      "Loss at step 6860 is 2.4016964435577393\n",
      "Loss at step 6870 is 2.47521710395813\n",
      "Loss at step 6880 is 2.4859745502471924\n",
      "Loss at step 6890 is 2.55885648727417\n",
      "Loss at step 6900 is 2.435387134552002\n",
      "Loss at step 6910 is 2.4791104793548584\n",
      "Loss at step 6920 is 2.51900315284729\n",
      "Loss at step 6930 is 2.42655348777771\n",
      "Loss at step 6940 is 2.4769837856292725\n",
      "Loss at step 6950 is 2.454606294631958\n",
      "Loss at step 6960 is 2.488769769668579\n",
      "Loss at step 6970 is 2.458711862564087\n",
      "Loss at step 6980 is 2.663256883621216\n",
      "Loss at step 6990 is 2.5627834796905518\n",
      "Loss at step 7000 is 2.559119462966919\n",
      "Loss at step 7010 is 2.5194311141967773\n",
      "Loss at step 7020 is 2.4379959106445312\n",
      "Loss at step 7030 is 2.496319055557251\n",
      "Loss at step 7040 is 2.4484004974365234\n",
      "Loss at step 7050 is 2.4907145500183105\n",
      "Loss at step 7060 is 2.50004243850708\n",
      "Loss at step 7070 is 2.382293462753296\n",
      "Loss at step 7080 is 2.5579237937927246\n",
      "Loss at step 7090 is 2.4586451053619385\n",
      "Loss at step 7100 is 2.5339975357055664\n",
      "Loss at step 7110 is 2.3634257316589355\n",
      "Loss at step 7120 is 2.352353572845459\n",
      "Loss at step 7130 is 2.418745279312134\n",
      "Loss at step 7140 is 2.6544601917266846\n",
      "Loss at step 7150 is 2.5109119415283203\n",
      "Loss at step 7160 is 2.5850327014923096\n",
      "Loss at step 7170 is 2.3718466758728027\n",
      "Loss at step 7180 is 2.595773696899414\n",
      "Loss at step 7190 is 2.460533618927002\n",
      "Loss at step 7200 is 2.441178798675537\n",
      "Loss at step 7210 is 2.486555814743042\n",
      "Loss at step 7220 is 2.571455717086792\n",
      "Loss at step 7230 is 2.453366994857788\n",
      "Loss at step 7240 is 2.4785866737365723\n",
      "Loss at step 7250 is 2.4885811805725098\n",
      "Loss at step 7260 is 2.586677312850952\n",
      "Loss at step 7270 is 2.471843957901001\n",
      "Loss at step 7280 is 2.6443352699279785\n",
      "Loss at step 7290 is 2.391122579574585\n",
      "Loss at step 7300 is 2.542642116546631\n",
      "Loss at step 7310 is 2.506605863571167\n",
      "Loss at step 7320 is 2.553102731704712\n",
      "Loss at step 7330 is 2.5223305225372314\n",
      "Loss at step 7340 is 2.4430224895477295\n",
      "Loss at step 7350 is 2.477912425994873\n",
      "Loss at step 7360 is 2.431300163269043\n",
      "Loss at step 7370 is 2.4717438220977783\n",
      "Loss at step 7380 is 2.469620943069458\n",
      "Loss at step 7390 is 2.6680235862731934\n",
      "Loss at step 7400 is 2.472688913345337\n",
      "Loss at step 7410 is 2.422125816345215\n",
      "Loss at step 7420 is 2.4686801433563232\n",
      "Loss at step 7430 is 2.491184711456299\n",
      "Loss at step 7440 is 2.397191047668457\n",
      "Loss at step 7450 is 2.544783115386963\n",
      "Loss at step 7460 is 2.4023306369781494\n",
      "Loss at step 7470 is 2.4455718994140625\n",
      "Loss at step 7480 is 2.628122568130493\n",
      "Loss at step 7490 is 2.515944242477417\n",
      "Loss at step 7500 is 2.3998939990997314\n",
      "Loss at step 7510 is 2.4479098320007324\n",
      "Loss at step 7520 is 2.510593891143799\n",
      "Loss at step 7530 is 2.4534718990325928\n",
      "Loss at step 7540 is 2.4768319129943848\n",
      "Loss at step 7550 is 2.497633934020996\n",
      "Loss at step 7560 is 2.4750630855560303\n",
      "Loss at step 7570 is 2.5067949295043945\n",
      "Loss at step 7580 is 2.436960458755493\n",
      "Loss at step 7590 is 2.4747681617736816\n",
      "Loss at step 7600 is 2.4612789154052734\n",
      "Loss at step 7610 is 2.4664270877838135\n",
      "Loss at step 7620 is 2.4867682456970215\n",
      "Loss at step 7630 is 2.3551881313323975\n",
      "Loss at step 7640 is 2.4196858406066895\n",
      "Loss at step 7650 is 2.5220866203308105\n",
      "Loss at step 7660 is 2.4842145442962646\n",
      "Loss at step 7670 is 2.4402453899383545\n",
      "Loss at step 7680 is 2.5604403018951416\n",
      "Loss at step 7690 is 2.4407694339752197\n",
      "Loss at step 7700 is 2.5607287883758545\n",
      "Loss at step 7710 is 2.5592401027679443\n",
      "Loss at step 7720 is 2.317054033279419\n",
      "Loss at step 7730 is 2.4209842681884766\n",
      "Loss at step 7740 is 2.443535804748535\n",
      "Loss at step 7750 is 2.4745538234710693\n",
      "Loss at step 7760 is 2.58174467086792\n",
      "Loss at step 7770 is 2.4888696670532227\n",
      "Loss at step 7780 is 2.4751687049865723\n",
      "Loss at step 7790 is 2.441197156906128\n",
      "Loss at step 7800 is 2.4737184047698975\n",
      "Loss at step 7810 is 2.4862828254699707\n",
      "Loss at step 7820 is 2.4638848304748535\n",
      "Loss at step 7830 is 2.5937318801879883\n",
      "Loss at step 7840 is 2.5432591438293457\n",
      "Loss at step 7850 is 2.5636510848999023\n",
      "Loss at step 7860 is 2.538393974304199\n",
      "Loss at step 7870 is 2.4429008960723877\n",
      "Loss at step 7880 is 2.528160572052002\n",
      "Loss at step 7890 is 2.486114501953125\n",
      "Loss at step 7900 is 2.4833157062530518\n",
      "Loss at step 7910 is 2.515052318572998\n",
      "Loss at step 7920 is 2.448178291320801\n",
      "Loss at step 7930 is 2.487732410430908\n",
      "Loss at step 7940 is 2.4515039920806885\n",
      "Loss at step 7950 is 2.446845531463623\n",
      "Loss at step 7960 is 2.3782222270965576\n",
      "Loss at step 7970 is 2.426398754119873\n",
      "Loss at step 7980 is 2.415022850036621\n",
      "Loss at step 7990 is 2.5226986408233643\n",
      "Loss at step 8000 is 2.4317171573638916\n",
      "Loss at step 8010 is 2.4259278774261475\n",
      "Loss at step 8020 is 2.539982795715332\n",
      "Loss at step 8030 is 2.4654269218444824\n",
      "Loss at step 8040 is 2.5135371685028076\n",
      "Loss at step 8050 is 2.494699001312256\n",
      "Loss at step 8060 is 2.5049071311950684\n",
      "Loss at step 8070 is 2.4272425174713135\n",
      "Loss at step 8080 is 2.370184898376465\n",
      "Loss at step 8090 is 2.463017463684082\n",
      "Loss at step 8100 is 2.4242374897003174\n",
      "Loss at step 8110 is 2.621429920196533\n",
      "Loss at step 8120 is 2.528803586959839\n",
      "Loss at step 8130 is 2.4101345539093018\n",
      "Loss at step 8140 is 2.4602773189544678\n",
      "Loss at step 8150 is 2.498112440109253\n",
      "Loss at step 8160 is 2.5739710330963135\n",
      "Loss at step 8170 is 2.568671703338623\n",
      "Loss at step 8180 is 2.4830780029296875\n",
      "Loss at step 8190 is 2.42767071723938\n",
      "Loss at step 8200 is 2.5080718994140625\n",
      "Loss at step 8210 is 2.476701259613037\n",
      "Loss at step 8220 is 2.5128049850463867\n",
      "Loss at step 8230 is 2.479844808578491\n",
      "Loss at step 8240 is 2.514667272567749\n",
      "Loss at step 8250 is 2.4411280155181885\n",
      "Loss at step 8260 is 2.453901767730713\n",
      "Loss at step 8270 is 2.49316143989563\n",
      "Loss at step 8280 is 2.389653444290161\n",
      "Loss at step 8290 is 2.3955931663513184\n",
      "Loss at step 8300 is 2.5028491020202637\n",
      "Loss at step 8310 is 2.5200653076171875\n",
      "Loss at step 8320 is 2.493119478225708\n",
      "Loss at step 8330 is 2.621840000152588\n",
      "Loss at step 8340 is 2.472003221511841\n",
      "Loss at step 8350 is 2.4680750370025635\n",
      "Loss at step 8360 is 2.5400218963623047\n",
      "Loss at step 8370 is 2.4781901836395264\n",
      "Loss at step 8380 is 2.387869358062744\n",
      "Loss at step 8390 is 2.513996124267578\n",
      "Loss at step 8400 is 2.5370898246765137\n",
      "Loss at step 8410 is 2.453395128250122\n",
      "Loss at step 8420 is 2.463061571121216\n",
      "Loss at step 8430 is 2.591688632965088\n",
      "Loss at step 8440 is 2.4658656120300293\n",
      "Loss at step 8450 is 2.3073487281799316\n",
      "Loss at step 8460 is 2.447432279586792\n",
      "Loss at step 8470 is 2.450657844543457\n",
      "Loss at step 8480 is 2.445615530014038\n",
      "Loss at step 8490 is 2.622333288192749\n",
      "Loss at step 8500 is 2.455533504486084\n",
      "Loss at step 8510 is 2.388622760772705\n",
      "Loss at step 8520 is 2.4956700801849365\n",
      "Loss at step 8530 is 2.5385496616363525\n",
      "Loss at step 8540 is 2.4493179321289062\n",
      "Loss at step 8550 is 2.4565296173095703\n",
      "Loss at step 8560 is 2.555756092071533\n",
      "Loss at step 8570 is 2.3386993408203125\n",
      "Loss at step 8580 is 2.4763221740722656\n",
      "Loss at step 8590 is 2.3598501682281494\n",
      "Loss at step 8600 is 2.3779146671295166\n",
      "Loss at step 8610 is 2.5057060718536377\n",
      "Loss at step 8620 is 2.5206034183502197\n",
      "Loss at step 8630 is 2.5062813758850098\n",
      "Loss at step 8640 is 2.512326717376709\n",
      "Loss at step 8650 is 2.4780969619750977\n",
      "Loss at step 8660 is 2.3009989261627197\n",
      "Loss at step 8670 is 2.5199599266052246\n",
      "Loss at step 8680 is 2.4906067848205566\n",
      "Loss at step 8690 is 2.461682081222534\n",
      "Loss at step 8700 is 2.484205961227417\n",
      "Loss at step 8710 is 2.540022134780884\n",
      "Loss at step 8720 is 2.44626522064209\n",
      "Loss at step 8730 is 2.4425442218780518\n",
      "Loss at step 8740 is 2.4084696769714355\n",
      "Loss at step 8750 is 2.5036275386810303\n",
      "Loss at step 8760 is 2.500453233718872\n",
      "Loss at step 8770 is 2.2952749729156494\n",
      "Loss at step 8780 is 2.512387275695801\n",
      "Loss at step 8790 is 2.558276891708374\n",
      "Loss at step 8800 is 2.456571102142334\n",
      "Loss at step 8810 is 2.557516098022461\n",
      "Loss at step 8820 is 2.4334776401519775\n",
      "Loss at step 8830 is 2.3681750297546387\n",
      "Loss at step 8840 is 2.4482462406158447\n",
      "Loss at step 8850 is 2.61672306060791\n",
      "Loss at step 8860 is 2.4741714000701904\n",
      "Loss at step 8870 is 2.5690605640411377\n",
      "Loss at step 8880 is 2.360442876815796\n",
      "Loss at step 8890 is 2.4244542121887207\n",
      "Loss at step 8900 is 2.5165152549743652\n",
      "Loss at step 8910 is 2.5325369834899902\n",
      "Loss at step 8920 is 2.512040376663208\n",
      "Loss at step 8930 is 2.413093328475952\n",
      "Loss at step 8940 is 2.5465762615203857\n",
      "Loss at step 8950 is 2.605883836746216\n",
      "Loss at step 8960 is 2.5085079669952393\n",
      "Loss at step 8970 is 2.5040760040283203\n",
      "Loss at step 8980 is 2.389362335205078\n",
      "Loss at step 8990 is 2.474923610687256\n",
      "Loss at step 9000 is 2.351151943206787\n",
      "Loss at step 9010 is 2.4816977977752686\n",
      "Loss at step 9020 is 2.5404579639434814\n",
      "Loss at step 9030 is 2.348062753677368\n",
      "Loss at step 9040 is 2.434797525405884\n",
      "Loss at step 9050 is 2.480057954788208\n",
      "Loss at step 9060 is 2.4483413696289062\n",
      "Loss at step 9070 is 2.445370674133301\n",
      "Loss at step 9080 is 2.4368107318878174\n",
      "Loss at step 9090 is 2.500218629837036\n",
      "Loss at step 9100 is 2.445625066757202\n",
      "Loss at step 9110 is 2.5715432167053223\n",
      "Loss at step 9120 is 2.3711249828338623\n",
      "Loss at step 9130 is 2.483898162841797\n",
      "Loss at step 9140 is 2.3889849185943604\n",
      "Loss at step 9150 is 2.3774349689483643\n",
      "Loss at step 9160 is 2.3023715019226074\n",
      "Loss at step 9170 is 2.4088706970214844\n",
      "Loss at step 9180 is 2.507093667984009\n",
      "Loss at step 9190 is 2.3805646896362305\n",
      "Loss at step 9200 is 2.4404587745666504\n",
      "Loss at step 9210 is 2.4374654293060303\n",
      "Loss at step 9220 is 2.4190866947174072\n",
      "Loss at step 9230 is 2.5388906002044678\n",
      "Loss at step 9240 is 2.414374351501465\n",
      "Loss at step 9250 is 2.517305374145508\n",
      "Loss at step 9260 is 2.4883251190185547\n",
      "Loss at step 9270 is 2.4130303859710693\n",
      "Loss at step 9280 is 2.463677406311035\n",
      "Loss at step 9290 is 2.423657178878784\n",
      "Loss at step 9300 is 2.4765493869781494\n",
      "Loss at step 9310 is 2.5092363357543945\n",
      "Loss at step 9320 is 2.4609572887420654\n",
      "Loss at step 9330 is 2.4569051265716553\n",
      "Loss at step 9340 is 2.458760976791382\n",
      "Loss at step 9350 is 2.465838670730591\n",
      "Loss at step 9360 is 2.4268925189971924\n",
      "Loss at step 9370 is 2.4286630153656006\n",
      "Loss at step 9380 is 2.5819613933563232\n",
      "Loss at step 9390 is 2.483362913131714\n",
      "Loss at step 9400 is 2.4461135864257812\n",
      "Loss at step 9410 is 2.5467824935913086\n",
      "Loss at step 9420 is 2.3150458335876465\n",
      "Loss at step 9430 is 2.430025100708008\n",
      "Loss at step 9440 is 2.57857608795166\n",
      "Loss at step 9450 is 2.5398876667022705\n",
      "Loss at step 9460 is 2.481335401535034\n",
      "Loss at step 9470 is 2.5633349418640137\n",
      "Loss at step 9480 is 2.5112831592559814\n",
      "Loss at step 9490 is 2.5056724548339844\n",
      "Loss at step 9500 is 2.3766567707061768\n",
      "Loss at step 9510 is 2.501530647277832\n",
      "Loss at step 9520 is 2.3379387855529785\n",
      "Loss at step 9530 is 2.4196462631225586\n",
      "Loss at step 9540 is 2.382866382598877\n",
      "Loss at step 9550 is 2.5021607875823975\n",
      "Loss at step 9560 is 2.3847336769104004\n",
      "Loss at step 9570 is 2.5272464752197266\n",
      "Loss at step 9580 is 2.433431386947632\n",
      "Loss at step 9590 is 2.432410717010498\n",
      "Loss at step 9600 is 2.62873911857605\n",
      "Loss at step 9610 is 2.4984450340270996\n",
      "Loss at step 9620 is 2.5701706409454346\n",
      "Loss at step 9630 is 2.470860004425049\n",
      "Loss at step 9640 is 2.4661006927490234\n",
      "Loss at step 9650 is 2.4327406883239746\n",
      "Loss at step 9660 is 2.428844690322876\n",
      "Loss at step 9670 is 2.4749464988708496\n",
      "Loss at step 9680 is 2.49242901802063\n",
      "Loss at step 9690 is 2.4094040393829346\n",
      "Loss at step 9700 is 2.4217238426208496\n",
      "Loss at step 9710 is 2.580531120300293\n",
      "Loss at step 9720 is 2.3645853996276855\n",
      "Loss at step 9730 is 2.4292404651641846\n",
      "Loss at step 9740 is 2.4729385375976562\n",
      "Loss at step 9750 is 2.4785079956054688\n",
      "Loss at step 9760 is 2.4270873069763184\n",
      "Loss at step 9770 is 2.4945480823516846\n",
      "Loss at step 9780 is 2.4740612506866455\n",
      "Loss at step 9790 is 2.5649378299713135\n",
      "Loss at step 9800 is 2.517151355743408\n",
      "Loss at step 9810 is 2.442885637283325\n",
      "Loss at step 9820 is 2.4597442150115967\n",
      "Loss at step 9830 is 2.3580641746520996\n",
      "Loss at step 9840 is 2.555675745010376\n",
      "Loss at step 9850 is 2.3764572143554688\n",
      "Loss at step 9860 is 2.555589199066162\n",
      "Loss at step 9870 is 2.4996337890625\n",
      "Loss at step 9880 is 2.5028257369995117\n",
      "Loss at step 9890 is 2.379272937774658\n",
      "Loss at step 9900 is 2.4991164207458496\n",
      "Loss at step 9910 is 2.482679605484009\n",
      "Loss at step 9920 is 2.401654005050659\n",
      "Loss at step 9930 is 2.5852651596069336\n",
      "Loss at step 9940 is 2.3577992916107178\n",
      "Loss at step 9950 is 2.5479321479797363\n",
      "Loss at step 9960 is 2.466399908065796\n",
      "Loss at step 9970 is 2.3968124389648438\n",
      "Loss at step 9980 is 2.487762689590454\n",
      "Loss at step 9990 is 2.3742516040802\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(f'Loss at step {steps} is {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Werorreltcod w f heatug t my't care!\n",
      "Adle, cld f sthen:\n",
      "AThemy TI my ne fu sthe wet:\n",
      "\n",
      "HERDaienothichevee t sinom me d:\n",
      "BOMNod he ak, he we m, eengmor t\n",
      "Wesivesp, d\n",
      "WWhen\n",
      "Ththithart overt musthe hes hogie thake:\n",
      "Sn bend swe buriman ithea\n",
      "Clovert it ck;\n",
      "Nos y ts bag,\n",
      "\n",
      "EWhed st sus d we aves! br:\n",
      "O:\n",
      "Th\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, 300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mathematical trick in self-attention\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones((T,T)))\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow3, xbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single head of self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, C) -> (B, T, head_size)\n",
    "q = query(x) # (B, T, C) -> (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T) \n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros(T, T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x) # (B, T, C) -> (B, T, head_size)\n",
    "\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
